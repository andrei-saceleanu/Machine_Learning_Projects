{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1597745684348}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZRO04WE46ssn","colab_type":"text"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"wE2UVmty6ssr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745074478,"user_tz":-180,"elapsed":582,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bv1DeC8X6ss3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745076218,"user_tz":-180,"elapsed":684,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71fnD3Cp6stA","colab_type":"text"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"85UBMlKN6stD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1597745078305,"user_tz":-180,"elapsed":652,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5f74afd0-0018-4f41-9579-f47b58a49eec"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f252338c240>"]},"metadata":{"tags":[]},"execution_count":27},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"h-huBJtZ6stK","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"EkRC7H3C6stL","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"WBpB0R4-6stM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1597745080723,"user_tz":-180,"elapsed":663,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"bb4b53bc-7acf-4f81-a436-acd5b0bb67d5"},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BwecsFK86stT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745083799,"user_tz":-180,"elapsed":688,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVqy3PCi6stb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745149020,"user_tz":-180,"elapsed":734,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","#<YOUR CODE: define network graph using raw TF, Keras, or any other library you prefer>\n","network=Sequential()\n","network.add(Dense(units=128,input_shape=state_dim,activation=\"relu\"))\n","network.add(Dense(units=64,activation=\"relu\"))\n","network.add(Dense(units=32,activation=\"relu\"))\n","network.add(Dense(units=16,activation=\"relu\"))\n","network.add(Dense(units=n_actions,activation=\"linear\"))\n","logits = network(ph_states)#<YOUR CODE: symbolic outputs of your network _before_ softmax>\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"_K7la59c6sti","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745151890,"user_tz":-180,"elapsed":705,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nHaIVhJ6str","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745153120,"user_tz":-180,"elapsed":554,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSkdtG4D6stx","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"DcjAD3BZ6stx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745521553,"user_tz":-180,"elapsed":640,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","\n","        # Sample action with given probabilities.\n","        #env.action_space.seed(seed=action_probs)\n","        a=np.random.choice(len(action_probs),p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"BTQEAEFh6st4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745522987,"user_tz":-180,"elapsed":715,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXLEBktv6st9","colab_type":"text"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"8fzBFtfh6st-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745524520,"user_tz":-180,"elapsed":691,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cumul=len(rewards)*[0]\n","    for i in range(len(rewards)-1,-1,-1):\n","      if i==len(rewards)-1:\n","        cumul[i]=rewards[i]\n","        continue\n","      cumul[i]=rewards[i]+gamma*cumul[i+1]\n","    return cumul"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ty0o4S-S_XCG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597745526269,"user_tz":-180,"elapsed":726,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"719c0fbf-91f8-4e2e-8638-69c34df99c52"},"source":["print(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9))"],"execution_count":51,"outputs":[{"output_type":"stream","text":["[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4PvTj0WW6suE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597745527294,"user_tz":-180,"elapsed":545,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4036c611-d469-4aad-b2fb-4f5047c4be92"},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":52,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HnCfrIMy6suJ","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"LpnL9d0N6suK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745529164,"user_tz":-180,"elapsed":725,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkE90Pwa6suO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745529983,"user_tz":-180,"elapsed":416,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean((log_policy_for_actions * ph_cumulative_rewards), axis=-1)"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7kTn_tn6suV","colab_type":"text"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"Gmqqwn-D6suW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745532920,"user_tz":-180,"elapsed":746,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","entropy =  -tf.reduce_mean(policy*log_policy)"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fOSZb0B6sua","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745533940,"user_tz":-180,"elapsed":665,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3bMzUn86sud","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745535618,"user_tz":-180,"elapsed":417,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkJ9WJfO6sui","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597745537120,"user_tz":-180,"elapsed":659,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OnYTnyx6sum","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"46PjDqW36sum","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597745588234,"user_tz":-180,"elapsed":49567,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"637cd11d-14e6-44fb-84d6-c3de9548be57"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":59,"outputs":[{"output_type":"stream","text":["mean reward: 22.300\n","mean reward: 39.130\n","mean reward: 86.220\n","mean reward: 58.800\n","mean reward: 346.360\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jtHyscTb6suq","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"CbFt3QTP6sur","colab_type":"code","colab":{}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mB1YVzjC6suv","colab_type":"code","colab":{}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vXviL5t6su0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597745663490,"user_tz":-180,"elapsed":21453,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"b9eb10ab-6c72-4a70-9289-3c93503d88f3"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'andrei.saceleanu@stud.acs.upb.ro', '7ABZ8gpehEseqKpu')"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Your average reward is 233.39 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m0gbpASJ6su3","colab_type":"text"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}